{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5231f4a",
   "metadata": {},
   "source": [
    "# Calling OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c596ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a64875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122f890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using `.env` files in Python applications is a common practice for managing environment variables and configuration settings without hardcoding them in your source code. The most popular way to do this is by using the `python-dotenv` library, which simplifies the process of loading environment variables from a `.env` file.\n",
      "\n",
      "Here's a step-by-step guide along with an example:\n",
      "\n",
      "### Step 1: Install `python-dotenv`\n",
      "\n",
      "If you don't have `python-dotenv` installed yet, you can install it using pip:\n",
      "\n",
      "```bash\n",
      "pip install python-dotenv\n",
      "```\n",
      "\n",
      "### Step 2: Create a `.env` File\n",
      "\n",
      "Create a file named `.env` in your project directory. This file should contain your environment variables in the format `KEY=VALUE`. For example:\n",
      "\n",
      "```plaintext\n",
      "# .env\n",
      "API_KEY=your_api_key_here\n",
      "DATABASE_URL=sqlite:///my_database.db\n",
      "DEBUG=True\n",
      "```\n",
      "\n",
      "### Step 3: Load the Environment Variables in Your Python Code\n",
      "\n",
      "You can now load the environment variables from the `.env` file in your Python script. Hereâ€™s an example:\n",
      "\n",
      "```python\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "# Load environment variables from the .env file\n",
      "load_dotenv()\n",
      "\n",
      "# Now you can access your environment variables using os.environ\n",
      "api_key = os.getenv('API_KEY')\n",
      "database_url = os.getenv('DATABASE_URL')\n",
      "debug_mode = os.getenv('DEBUG') == 'True'  # Convert to boolean\n",
      "\n",
      "# Example usage\n",
      "if debug_mode:\n",
      "    print(\"Debug mode is on\")\n",
      "print(f\"Using API key: {api_key}\")\n",
      "print(f\"Connecting to database at: {database_url}\")\n",
      "```\n",
      "\n",
      "### Step 4: Run Your Python Script\n",
      "\n",
      "Run your Python script as you normally would:\n",
      "\n",
      "```bash\n",
      "python your_script.py\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "- **Install `python-dotenv`:** This library will help you load environment variables from a `.env` file.\n",
      "- **Create a `.env` file:** Define your key-value pairs for configuration.\n",
      "- **Load variables in your script:** Use `load_dotenv()` and `os.getenv()` to access the variables.\n",
      "\n",
      "### Note\n",
      "\n",
      "Make sure not to commit your `.env` file containing sensitive information to version control (e.g., Git). You can add it to your `.gitignore` file to prevent accidental commits:\n",
      "\n",
      "```plaintext\n",
      "# .gitignore\n",
      ".env\n",
      "```\n",
      "\n",
      "This approach helps you keep your configuration secure and easily manageable across different environments (development, testing, production, etc.).\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "  api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  store=True,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"How can i use .env files with python example\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cf798",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c1f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search-engine in the search-engine folder installed via uv add.\n",
    "import minsearch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca22d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690fa65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# creating index\u001b[39;00m\n\u001b[1;32m      2\u001b[0m index \u001b[38;5;241m=\u001b[39m minsearch\u001b[38;5;241m.\u001b[39mIndex(\n\u001b[1;32m      3\u001b[0m             text_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqeustion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m             keyword_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Performing a search\u001b[39;00m\n\u001b[1;32m     11\u001b[0m q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe course has already started, can i still enroll?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projects/llm-zoomcamp/llm-zoomcamp/.venv/lib/python3.10/site-packages/minsearch/minsearch.py:51\u001b[0m, in \u001b[0;36mIndex.fit\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_fields:\n\u001b[1;32m     50\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mget(field, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_matrices[field] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyword_fields:\n",
      "File \u001b[0;32m~/Projects/llm-zoomcamp/llm-zoomcamp/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2103\u001b[0m )\n\u001b[0;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/llm-zoomcamp/llm-zoomcamp/.venv/lib/python3.10/site-packages/sklearn/base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1361\u001b[0m     )\n\u001b[1;32m   1362\u001b[0m ):\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/llm-zoomcamp/llm-zoomcamp/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             )\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/llm-zoomcamp/llm-zoomcamp/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m         )\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# creating index\n",
    "index = minsearch.Index(\n",
    "            text_fields=[\"qeustion\", \"text\", \"section\"],\n",
    "            keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n",
    "\n",
    "# Performing a search\n",
    "q=\"the course has already started, can i still enroll?\"\n",
    "\n",
    "boost = {\"question\": 3.0,\n",
    "         \"section\": 0.5,\n",
    "        }\n",
    "\n",
    "results = index.search(\n",
    "    query=q,\n",
    "    filter_dict= {'course': 'data-engineering-zoomcamp'},\n",
    "    boost_dict=boost,\n",
    "    num_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
